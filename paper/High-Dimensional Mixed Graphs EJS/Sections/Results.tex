\subsection{Simulation results: binary-continuous mix}

We start by considering a mix of binary and continuous variables generated as outlined in Section \ref{sec::setup} to be able to compare results with those of \citet{Fan17}. For this purpose, \figref{fig:bench_binary} depicts the mean estimation error $\Norm{\hat{\boldsymbol\Omega} - \boldsymbol\Omega^*}_F$ and the AUC for the different estimators under the different $(d,n)$ regimes.
%Table \ref{kendall_table} reports mean estimation errors $\Norm{\hat{\boldsymbol\Omega} - \boldsymbol\Omega^*}_F$ under the different $(d,n)$ regimes. When for all $j$, $f_j(x) = x$ we recover the latent Gaussian and when $f_j(x) = x^3$ the latent Gaussian copula model.

The oracle estimator in the third column of Table \ref{kendall_table} corresponds to estimating $\hat{\boldsymbol\Sigma}^{(n)}$ from Definition \ref{def1} based on realization of the latent data $(\boldsymbol{Z_1},\boldsymbol{Z_2})$. Next in column four, the binary $\hat{\boldsymbol\Omega}_\tau$ indicates the nonparanormal estimator proposed by \citet{Fan17}. The next two columns, namely $\hat{\boldsymbol\Omega}_{\text{MLE}}$ and $\hat{\boldsymbol\Omega}_r$ indicate the ML approach and the general mixed estimator developed in Sections \ref{sec::latent_gaussian} and \ref{sec::nonparanormal}, respectively.
%\input{Tables/binary_table}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/binary.pdf}
    \caption{Binary benchmark results}
    \label{fig:bench_binary}
\end{figure}

As expected, the estimation error for $d = 50$ and $n=200$ in the latent Gaussian setting is almost identical for $\hat{\boldsymbol\Omega}_\tau$ and our proposed nonparanormal estimator $\hat{\boldsymbol\Omega}_r$. Additionally, $\hat{\boldsymbol\Omega}_{\text{MLE}}$ performs best in this case. Compared to the oracle estimator, all three approaches exhibit very little loss in accuracy that arises due to binarization. Looking at graph recovery in terms of FPR, TPR, and AUC the picture is similar.
%For $\hat{\boldsymbol\Omega}_\tau$, $\hat{\boldsymbol\Omega}_{\text{MLE}}$, and $\hat{\boldsymbol\Omega}_r$ the FPR is around $22-25\%$ while the TPR amounts to $56-58\%$. Together, they form the AUC amounting to around $0.71$. Note, that the oracle estimator produces fewer TPs at a much lower FPR. The reason for the relatively low TPR is the fact that the glasso penalty induces too much sparsity in this low-dimensional setting. On the other hand, $\hat{\boldsymbol\Omega}_\tau$, $\hat{\boldsymbol\Omega}_{\text{MLE}}$, and $\hat{\boldsymbol\Omega}_r$ seem to be less conservative and produce more FPs.

Turning to the nonparanormal setting with $f_j(x) = x^3$ under $d = 50$ and $n=200$, as expected $\hat{\boldsymbol\Omega}_\tau$ and $\hat{\boldsymbol\Omega}_r$ remain largely unchanged but for small numerical differences. However, for $\hat{\boldsymbol\Omega}_{\text{MLE}}$ accuracy, both in terms of estimation error and graph recovery drops noticeably. However, the FPR remains unchanged, indicating that whilst detecting fewer correct edges in the graph the number of incorrect edges is not affected by this particular transformation. When increasing the number of variables to $d=250$ and $d=750$ the picture is similar to before. To sum up, when compared to the estimator $\hat{\boldsymbol\Omega}_\tau$ proposed by \citet{Fan17} both $\hat{\boldsymbol\Omega}_{\text{MLE}}$ and $\hat{\boldsymbol\Omega}_r$ perform similarly, under the latent Gaussian assumption even somewhat better. $\hat{\boldsymbol\Omega}_r$ performs slightly better than $\hat{\boldsymbol\Omega}_\tau$ in all scenarios considered. The loss of accuracy that arises from the discretization is not too severe under all $(d,n)$ regimes considered. Note, that the binary-continuous mix is merely a special case of the general mixed data scheme we consider in our paper. Therefore, being able to show similar or even improved performance to the current gold standard is important.

\subsection{Arbitrary mixed data results}

We turn to the set of experiments where we generate a mix of continuous and discrete data with arbitrary numbers of levels. As existing approaches, and in particular the bridge function approach, do not extend to this fully general case, we can no longer compare the proposed method to an existing one. Instead in Table \ref{mix_table}, we report a second oracle estimator, namely oracle $\hat{\Omega}_\rho$, i.e. applying Definition \ref{case1_nonpara} to realization from $\boldsymbol{Z} = (\boldsymbol{Z}_1, \boldsymbol{X}_2)$ in order to get more insight into the cases where $f_j(x) = x^3$.

When comparing $\Norm{\hat{\boldsymbol\Omega} - \boldsymbol\Omega^*}_F$ across the different settings, similar to the previous results, efficiency loss is almost negligible.
\input{Tables/general_table}
\noindent This is true both for $\hat{\boldsymbol\Omega}_r$ and for $\hat{\boldsymbol\Omega}_{\text{MLE}}$ in the latent Gaussian settings. Furthermore, graph recovery in terms of AUC improves overall owing to the fact that more information regarding the latent variable is available.
%This indicates that having more diverse level-sets increases the information regarding the latent variables, rather than having a negative impact on performance. 
All remaining characteristics established in the previous scenario translate to the general case. In the nonparanormal settings $\hat{\boldsymbol\Omega}_{\text{MLE}}$ fails to establishing true positives rather than producing more FPs.
%While in the low and medium dimensional setting too much sparsity is imposed in the high dimensional scenario FPRs of $\hat{\boldsymbol\Omega}_r$ and $\hat{\boldsymbol\Omega}_{\text{MLE}}$ are lower than for both oracle estimators confirming that they are more conservative in terms of graph recovery. 

In conclusion, the results of the simulation study reveal that both estimators developed in this paper perform favorably when compared to the state of the art. Particularly, the nonparanormal estimator $\hat{\boldsymbol\Omega}_r$
performs well, but is simple, removing the need to derive potentially large numbers of bridge functions to generalize the setting in \citet{Fan17} further. Instead, the polychoric and polyserial correlations agree naturally with the latent Gaussian copula model.