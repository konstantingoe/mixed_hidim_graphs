To numerically assess the accuracy of our mixed graph estimation approach, we commence with a simulation study in which the estimators are rigorously evaluated in a gold-standard fashion and compared against oracles.

\subsection{Simulation setup}\label{sec::setup}

% To foster comparability, we adopt a data-generating strategy akin to that of \cite{Fan17}. Our experiments unfold in two parts. First, we investigate the binary mixed case, benchmarking our approach against the transformation functions being identity and cubed, respectively; see scenarios (c) and (d) in Section 6.1 of   \cite{Fan17}. Second, we consider the general mixed case where the discrete variables are allowed to have arbitrarily many levels. In this case, we compare our approach to the ensemble approach by \cite{Feng19}.

% Subsequently, we generate a combination of binary-ternary-continuous data. Despite the absence of numerical results in the simulation study by \citet{Quan18}, we compare our approach with their extension of \citet{Fan17}. In the third part, we generate discrete data with varying levels from $\mathbf{Z}$ and assess performance against\todo{the ensemble estimator proposed by \cite{Feng19}} the latent continuous oracle.

% For detailed results and a comprehensive description of the simulation setup for binary-ternary-continuous data simulations, refer to Section 6\todo{Move this setup from the appendix to the plain text.} of the Supplementary Materials.


We start by constructing the underlying precision matrix $\mathbf{\Omega}^*$ whose zero pattern encodes the undirected graph. We set $\mathbf{\Omega}^*_{jj} = 1$ and $\mathbf{\Omega}_{jk}^* = s \cdot b_{jk}$ if $j\neq k$, where $s$ is a constant signal strength chosen to assure positive definiteness. Furthermore, $b_{jk}$ are realizations of a Bernoulli random variable with corresponding success probability $p_{jk} = (2\pi)^{-1/2} \exp\big[\Norm{v_j - v_k}_2 / (2c) \big]$. In particular, $v_j = (v_j^{(1)}, v_j^{(2)})$ are independent realizations of a bivariate uniform $[0,1]$ distribution and $c$ controls the sparsity of the graph.

Throughout the simulation, we set $s = 0.15$ and incrementally increase the dimensionality s.t. $d \in \{50,250,750\}$, representing a transition from small to large-scale graphs. We let $\mathbf{\Sigma}^* = (\mathbf{\Omega}^*)^{-1}$ be rescaled such that all diagonal elements are equal to $1$. Given $\mathbf\Sigma^*$, we first obtain the partially latent continuous data \(\mathbf{Z} = (\mathbf{Z}_1, \mathbf{X}_2)\) where $\mathbf{Z} \sim \text{NPN}_d(\mathbf{0}, \mathbf{\Sigma}^*, f)$. In practice, we draw \(n\) i.i.d. samples from \(\text{N}_d(\mathbf{0}, \mathbf{\Sigma}^*)\) and apply the back-transform \(f^{-1}\) to each individual variable.

To generate general mixed data \(\mathbf{X} = (\mathbf{X}_1,\mathbf{X}_2)\) according to the LGCM we need to appropriately threshold \(\mathbf{Z}_1\). Let $\mathbf{X}_1$ be partitioned into equally sized collections of binary, ordinal, and Poisson distributed random variables, i.e., $\mathbf{X}_1 = (\mathbf{X}_1^{\text{bin}}, \mathbf{X}_1^{\text{ord}},\mathbf{X}_1^{\text{pois}})$. We use the inverse probability integral transform (IPT) to generate random samples from the respective cumulative distribution functions, corresponding to the relationship described in Eq. \eqref{latent_ordered}. For $\mathbf{X}_1^{\text{bin}}$ IPT is employed with success probability drawn from Uniform$[0.4,0.6]$ for $80\%$ of $\mathbf{X}_1^{\text{bin}}$. We assign unbalanced classes to the remaining $20\%$, where the success probability is drawn from Uniform$[0.05,0.1]$. Regarding $\mathbf{X}_1^{\text{ord}}$, IPT is used to generate samples from the multinomial distribution. To that end, we draw the number of categories from Uniform$[3,7]$ and round it to the nearest integer. We set the probability of falling into one of these categories to be proportional to their number. Lastly, $\mathbf{X}_1^{\text{pois}}$ is generated using IPT with the rate parameter set to $6$. In case we only need a mix of binary and continuous data, we set \(\mathbf{X}_1 = \mathbf{X}_1^{\text{bin}}\).

Throughout the experiments, $\hat{\Omega}$ is chosen by minimizing the eBIC according to the procedure outlined in Section \ref{sec:precision_matrix} with $\theta = 0.1$ for the low and medium, $\theta = 0.5$ for the high dimensional graphs.
The sample size $n$ is set to \(200\) for \(d\in \{50, 250\}\) and \(300\) for \(d = 750\). We set the number of simulation runs to \(100\). Lastly, we choose the sparsity parameter $c$ such that the number of edges aligns roughly with the dimension -- except for $d = 50$, where we allow for $200$ edges following \citet{Fan17}.

\paragraph{Performance metrics.}
% To assess performance, we report the mean estimation error $\Norm{\hat{\mathbf{\Omega}} - \mathbf\Omega^*}_F$ as evaluated by the Frobenius norm. Furthermore, we consider graph recovery metrics. To this end, we define the number of true positives $\text{TP}(\lambda)$ and false positives $\text{FP}(\lambda)$ depending on the \textit{glasso path} as the number of nonzero lower off-diagonal elements that agree both in $\mathbf\Omega^*$ and $\hat{\mathbf\Omega}$ and the number of nonzero lower off-diagonal elements in $\hat{\mathbf\Omega}$ that are actually zero in $\mathbf\Omega^*$, respectively. The true positive rate $\text{TPR}(\lambda)$ and the false positive rate $\text{FPR}(\lambda)$ are defined as $\text{TPR} = \frac{\text{TP}(\lambda)}{\abs{E}} $ and $\text{FPR} = \frac{\text{FP}(\lambda)}{d(d-1)/2 - \abs{E}}$, respectively. Lastly, we consider the area under the curve (AUC) where a value of $0.5$ corresponds to random guessing of the presence of an edge and a value of $1$ corresponds to perfect error-free recovery of the underlying latent graph (in the rank sense of ROC analysis).
To evaluate performance, we report the mean estimation error $\Norm{\hat{\mathbf{\Omega}} - \mathbf\Omega^*}_F$ using the Frobenius norm. Additionally, we employ graph recovery metrics. For this purpose, we calculate the number of true positives $\text{TP}(\lambda)$ and false positives $\text{FP}(\lambda)$ based on the \textit{glasso path}. $\text{TP}(\lambda)$ represents the count of non-zero lower off-diagonal elements that are consistent both in $\mathbf\Omega^*$ and $\hat{\mathbf\Omega}$, while $\text{FP}(\lambda)$ denotes the count of non-zero lower off-diagonal elements in $\hat{\mathbf\Omega}$ that are zero in $\mathbf\Omega^*$.

The true positive rate $\text{TPR}(\lambda)$ and false positive rate $\text{FPR}(\lambda)$ are defined as $\text{TPR} = \frac{\text{TP}(\lambda)}{\abs{E}}$ and $\text{FPR} = \frac{\text{FP}(\lambda)}{d(d-1)/2 - \abs{E}}$, respectively. Finally, we consider the area under the curve (AUC), where a value of $0.5$ corresponds to random guessing of edge presence and a value of $1$ indicates perfect error-free recovery of the underlying latent graph (in the rank sense of ROC analysis).